{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Audio Clip Extraction\n",
    "\n",
    "The neural net model we want to end up with will listen to live audio (from the host device's microphone) to identify, in real-time, which musical notes are being played.\n",
    "\n",
    "To do this, we'll feed it small buffers of audio as we receive them from the audio API. Typical buffer sizes range from a few hundred to a few thousand audio samples. Traditionally, the buffer size is some power of two.\n",
    "\n",
    "---\n",
    "\n",
    "### Terminology sidebar: \"sample\"\n",
    "\n",
    "When discussing the training of a neural net, a *sample* is entry from a training or testing data set. These are sometime called \"feature vectors\". In the context of MAPS, it is one .wav file, with its partnter .txt and .mid files.\n",
    "\n",
    "When discussing digital audio, a *sample* represent the amplitude of a digitised sound at a particular time. For example, a digitising *sample rate* of 44,100Hz would result in 44,100 *samples* every second.\n",
    "\n",
    "We can usually tell from context which meaning of the world \"sample\" is intended, but - when needed - we can use **feature sample** and **audio sample** to disambiguate.\n",
    "\n",
    "---\n",
    "\n",
    "## Web Audio API audio format\n",
    "\n",
    "When working with the Web Audio API:\n",
    "\n",
    "1. We can choose the sample rate we would like.\n",
    "2. We can choose to receive mono or stereo data.\n",
    "3. Audio samples are stored as floats ranging from -1 to +1.\n",
    "\n",
    "Because note detection doesn't need stero - and to reduce processing - we'll ask the API for mono data.\n",
    "\n",
    "\n",
    "## MAPS audio format\n",
    "\n",
    "According to the MAPS documentation:\n",
    "\n",
    "1. MAPS feature samples have been digitised at 44,100Hz.\n",
    "2. They are stereo. That is, they contain two \"channels\" of audio data.\n",
    "3. Each audio sample is stored as a signed, 16-bit integer.\n",
    "\n",
    "Also, when we use `scipy.io.wavfile.read` to read the .wav files, the stereo channels are interleaved. In other words, three audio samples from the left and right channels are stored as `[[L0, R0], [L1, R1], [L2, R2]`.\n",
    "\n",
    "## Our preferred audio format\n",
    "\n",
    "The highest note on an 88-key piano is C8. This note has a frequency of approximately 4.2 kHz. By the [Nyquistâ€“Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem), we need a sample rate of 8.4 kHz or higher to accurately capture C8's fundamental frequency.\n",
    "\n",
    "It is tempting to use a low sample rate, to reduce the computational load of note detection, but it is important to remember that the note's harmonics carry a lot of useful information as well. C8's third harmonic (16.8 kHz) needs a sample rate of more than 32 kHz to capture accurately.\n",
    "\n",
    "So - for now, we will use:\n",
    "\n",
    "1. The MAPS sample rate of 44,100 Hz\n",
    "2. One of the stereo channels\n",
    "3. Audio samples stored as floats ranging from -1 to +1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes to make to `samples/maps.py`\n",
    "\n",
    "`read_samples` currently returns samples looking like this:\n",
    "\n",
    "```\n",
    "{'instrument': 'ENSTDkAm',\n",
    " 'sample': 'MAPS_ISOL_NO_F_S0_M100_ENSTDkAm',\n",
    " 'sample_rate': 44100,\n",
    " 'audio': array([[ 1,  9],\n",
    "        [ 0,  7],\n",
    "        [-1,  7],\n",
    "        ...,\n",
    "        [19, 19],\n",
    "        [18, 17],\n",
    "        [18, 18]], dtype=int16),\n",
    " 'notes': [{'onset': '0.51599', 'offset': '2.521', 'midi_pitch': '100'}]}\n",
    "```\n",
    "\n",
    "## Channels\n",
    "\n",
    "To easily extract one channel of audio from the MAPS samples, `read_sample` must be modified to *transpose* the audio data from `[[L0, R0], [L1, R1], [L2, R2]` to `[[L0, L1, L2]. [R0, R1, R2]]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  9]\n",
      " [ 0  7]\n",
      " [-1  7]\n",
      " [19 19]\n",
      " [18 17]\n",
      " [18 18]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "audio = np.array([[ 1,  9],\n",
    "        [ 0,  7],\n",
    "        [-1,  7],\n",
    "        [19, 19],\n",
    "        [18, 17],\n",
    "        [18, 18]])\n",
    "\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0 -1 19 18 18]\n",
      " [ 9  7  7 19 17 18]]\n"
     ]
    }
   ],
   "source": [
    "print(audio.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onset and offset times\n",
    "\n",
    "MAPS note data records onset and offset times in seconds. For our purposes, an audio sample index is more useful than a time value.\n",
    "\n",
    "We'll define a function `time_to_index` to do this conversion for us. Time values are expressed in seconds, with fractional values. Audio sample indices must be strictly integral. I find it useful to round onset times *up* and offset times *down*, so `time_to_index` will take an optional third parameter specifying the rounding function to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onset: 0.51599s => audio[22756]\n",
      "offset: 2.521s => audio[111176]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def time_to_index(sample_rate, seconds, round_function = round):\n",
    "    return round_function(sample_rate * seconds)\n",
    "\n",
    "note = {'onset': '0.51599', 'offset': '2.521', 'midi_pitch': '100'} # taken from the example above\n",
    "\n",
    "sample_rate = 44100\n",
    "onset_time = float(note['onset'])\n",
    "onset_index = time_to_index(sample_rate, onset_time, math.ceil)\n",
    "\n",
    "offset_time = float(note['offset'])\n",
    "offset_index = time_to_index(sample_rate, offset_time, math.floor)\n",
    "\n",
    "print(\"onset: {0}s => audio[{1}]\".format(onset_time, onset_index))\n",
    "print(\"offset: {0}s => audio[{1}]\".format(offset_time, offset_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note numbers\n",
    "\n",
    "MAPS (*MIDI* Aligned Piano Sounds) is, as the name says, MIDI based. They use MIDI pitch numbers to represent notes.\n",
    "\n",
    "In the Javascript libraries I've been developing for this project, I use a different numbering system.\n",
    "\n",
    "The function `midi_pitch_to_note_index` will do this translation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_pitch_to_note_index(midi_pitch):\n",
    "    # The note C0 has a MIDI pitch number of 12\n",
    "    # Within my music library, C0 has a \"note index\" of 1\n",
    "    # Both systems count in semitones, so there is a simple difference of 11 between them.\n",
    "    return midi_pitch - 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio sample window sizes\n",
    "\n",
    "As mentioned above, we'll be receiving small buffers of audio from the Web Audio API.\n",
    "\n",
    "These buffers are also called windows, and are what we'll be feeding into the neural net for note identification.\n",
    "\n",
    "We need to determine the best window size to use.\n",
    "\n",
    "The lowest note we are interested in is the \"bottom\" note of an 88-key piano. This is note the A0, with a fundamental frequency of 27.5 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 period:\n",
      "0.03636363636363636 seconds\n",
      "1603.6363636363635 samples\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 44100\n",
    "lowest_note_freq = 27.5\n",
    "\n",
    "lowest_note_period = 1 / lowest_note_freq\n",
    "\n",
    "print(\"A0 period:\")\n",
    "print(\"{0} seconds\".format(lowest_note_period))\n",
    "print(\"{0} samples\".format(lowest_note_period * sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period of A0's fundamental is about 0.036 seconds, or about 1604 audio samples.\n",
    "\n",
    "Any higher note would have a shorter period, so 1604 samples is enough to capture at least one full period of any piano note.\n",
    "\n",
    "To keep the app feeling responsive, we would like it to respond to a note or chord being played within about a tenth of a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 second => 4410 samples\n"
     ]
    }
   ],
   "source": [
    "max_window_seconds = 0.1\n",
    "max_window_samples = round(max_window_seconds * sample_rate)\n",
    "\n",
    "print(\"{0} second => {1} samples\".format(max_window_seconds, max_window_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the minimum window size we need is 1604 samples (0.036s), and the maximum we want is 4410 samples (0.1s)\n",
    "\n",
    "This gives us 2048 and 4096 as possible window sizes (remembering that window sizes should be a power of 2).\n",
    "\n",
    "For now, we'll use 4096 to have the best chance of success. Once we have a working model, we can try 2048 to see if it works as well as 4096 does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_samples = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting clips for training\n",
    "\n",
    "Eventually, we'll have some fancy clip extraction which extracts random portions of the audio data and which handles multiple notes being played simultaneously.\n",
    "\n",
    "For now - just to get started with a basic model - we'll only handle single notes, and will always extract a clip starting at the exact onset of the note.\n",
    "\n",
    "The function `get_onset_clip` will do this basic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'note': 89,\n",
       " 'clip': array([  -40,   -45,   -48, ..., -1100,  -688,  -252], dtype=int16)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from samples.maps import read_samples as read_maps_samples\n",
    "\n",
    "sample = next(read_maps_samples('/datasets/audio/maps'))\n",
    "\n",
    "def get_onset_clip(sample, channel, window_samples):\n",
    "    assert len(sample['notes']) == 1\n",
    "    note = sample['notes'][0]\n",
    "    sample_rate = sample['sample_rate']\n",
    "    onset_index = time_to_index(sample_rate, float(note['onset']), math.ceil)\n",
    "    \n",
    "    clip = sample['audio'].transpose()[channel][onset_index:onset_index + window_samples]\n",
    "    note_index = midi_pitch_to_note_index(int(note['midi_pitch']))\n",
    "    return {'note': note_index, 'clip': clip};\n",
    "\n",
    "\n",
    "get_onset_clip(sample, 0, window_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
