{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read MAPS Database\n",
    "\n",
    "This notebook represents my initial Python development to read piano audio samples from the \"MAPS Database\" and prepare them to be used as training data for a pitch-detection neural net.\n",
    "\n",
    "## The MAPS Database\n",
    "\n",
    "The MAPS (\"MIDI Aligned Piano Sounds\") Database is a collection of piano sounds, ranging from individual notes and chords to complete pieces of music.\n",
    "\n",
    "Each sound file is accompanied by the MIDI data that was used to produce it. This data acts as the \"ground truth\" for the sample.\n",
    "\n",
    "MAPS was produced by Valentin Emiya (Telecom ParisTech, 2008), and is available at [Telecom ParisTech's website](http://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/).\n",
    "\n",
    "## Layout of the Database Files\n",
    "\n",
    "The database consists of a collection of directories and files.\n",
    "\n",
    "The top level consists of a number of instruments, each having its own directory.\n",
    "\n",
    "Within each instrument's directory, a well-defined directory structure exists. For example, a set of *isol*ated notes, played *no*rmally exist within the \"ISOL/NO\" directory for each instrument. This directory structure is documented in the \"MAPS_doc.pdf\" file included within the database.\n",
    "\n",
    "## Initial Scope of Work\n",
    "\n",
    "At first, I will only be dealing with individual notes. Multi-note training will be handled later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The `maps_path` variable needs to be set to the root directory of the MAPS database. This is the directory that contains the instrument directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "from scipy.io import wavfile\n",
    "\n",
    "maps_path = '/datasets/audio/maps'\n",
    "isolated_notes_subpath = 'ISOL/NO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruments\n",
    "\n",
    "`list_instruments` is a generator returning the name and directory of each instrument found in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instrument': 'ENSTDkAm', 'directory': '/datasets/audio/maps\\\\ENSTDkAm'},\n",
       " {'instrument': 'StbgTGd2', 'directory': '/datasets/audio/maps\\\\StbgTGd2'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_instruments(root_directory):\n",
    "    files_and_dirs = ( (file_or_dir, os.path.join(root_directory, file_or_dir)) for file_or_dir in os.listdir(root_directory))\n",
    "    return ( {'instrument':name, 'directory': directory} for name, directory in files_and_dirs if os.path.isdir(directory))\n",
    "\n",
    "[instrument for instrument in list_instruments(maps_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample files\n",
    "\n",
    "Sample audio data is stored in `.wav` files.\n",
    "\n",
    "Each audio file is accompanied by a `.txt` file and a `.mid` file.\n",
    "\n",
    "The `.txt` file contains a list (in CSV format) of the notes present in the file.\n",
    "\n",
    "The `.mid` file is the MIDI file from which the audio was generated.\n",
    "\n",
    "At the moment, I don't use the MIDI file, so a \"sample\" is defined as a pair of a `.wav` file and a `.txt` file with the same base name.\n",
    "\n",
    "Given a directory (such as `<maps_root>/<instrument>/ISOL/NO`), `list_samples` generates a list of samples in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2',\n",
       " 'wav_file': '/datasets/audio/maps\\\\StbgTGd2\\\\ISOL/NO\\\\MAPS_ISOL_NO_F_S0_M100_StbgTGd2.wav',\n",
       " 'txt_file': '/datasets/audio/maps\\\\StbgTGd2\\\\ISOL/NO\\\\MAPS_ISOL_NO_F_S0_M100_StbgTGd2.txt'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_samples(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            root, _ = os.path.splitext(filename)\n",
    "            txt_filename = os.path.join(directory, root+'.txt')\n",
    "            if os.path.isfile(txt_filename):\n",
    "                yield {'sample':root, 'wav_file':os.path.join(directory, root+'.wav'), 'txt_file':txt_filename}\n",
    "                \n",
    "next(samples for samples in list_samples(os.path.join(maps_path, 'StbgTGd2', isolated_notes_subpath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read sample\n",
    "\n",
    "Given an item from `list_samples`, such as:\n",
    "\n",
    "    {'sample': 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2',\n",
    "     'wav_file': '<maps_root>/<instrument>ISOL/NO/MAPS_ISOL_NO_F_S0_M100_StbgTGd2.wav',\n",
    "     'txt_file': '<maps_root>/<instrument>ISOL/NO/MAPS_ISOL_NO_F_S0_M100_StbgTGd2.txt'}\n",
    "\n",
    "`read_sample` will read the audio data (using `read_sample_audio`) and the list of notes (using `read_sample_notes`), and will return an item such as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2',\n",
       " 'sample_rate': 44100,\n",
       " 'audio': array([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]], dtype=int16),\n",
       " 'notes': [{'onset': '0.500004', 'offset': '2.50001', 'midi_pitch': '100'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_sample_audio(wav_filename):\n",
    "    return scipy.io.wavfile.read(wav_filename)\n",
    "\n",
    "import csv\n",
    "\n",
    "def read_sample_notes(txt_filename):\n",
    "    with open(txt_filename) as file:\n",
    "        reader = csv.DictReader(file, dialect='excel-tab')\n",
    "        return [{'onset':note['OnsetTime'], 'offset':note['OffsetTime'], 'midi_pitch':note['MidiPitch']} for note in reader]\n",
    "\n",
    "def read_sample(sample):\n",
    "    sample_rate, audio = read_sample_audio(sample['wav_file'])\n",
    "    notes = read_sample_notes(sample['txt_file'])\n",
    "    return {'sample': sample['sample'],\n",
    "            'sample_rate': sample_rate,\n",
    "            'audio': audio,\n",
    "            'notes': notes\n",
    "           }\n",
    "\n",
    "read_sample({'sample': 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2',\n",
    "     'wav_file': os.path.join(maps_path, 'StbgTGd2', isolated_notes_subpath, 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2.wav'),\n",
    "     'txt_file': os.path.join(maps_path, 'StbgTGd2', isolated_notes_subpath, 'MAPS_ISOL_NO_F_S0_M100_StbgTGd2.txt')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "`read_samples` is a generator that, given the MAPS root directory, will produce entries with the following form:\n",
    "\n",
    "    instrument\n",
    "    sample\n",
    "    sample_rate\n",
    "    audio\n",
    "    notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instrument': 'ENSTDkAm',\n",
       " 'sample': 'MAPS_ISOL_NO_F_S0_M100_ENSTDkAm',\n",
       " 'sample_rate': 44100,\n",
       " 'audio': array([[ 1,  9],\n",
       "        [ 0,  7],\n",
       "        [-1,  7],\n",
       "        ...,\n",
       "        [19, 19],\n",
       "        [18, 17],\n",
       "        [18, 18]], dtype=int16),\n",
       " 'notes': [{'onset': '0.51599', 'offset': '2.521', 'midi_pitch': '100'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_samples(root_directory):\n",
    "    for instrument in list_instruments(root_directory):\n",
    "        for sample in list_samples(os.path.join(instrument['directory'], isolated_notes_subpath)):\n",
    "            read = read_sample(sample)\n",
    "            yield {'instrument': instrument['instrument'],\n",
    "                   'sample': sample['sample'],\n",
    "                   'sample_rate': read['sample_rate'],\n",
    "                   'audio': read['audio'],\n",
    "                   'notes': read['notes']\n",
    "                  }\n",
    "        \n",
    "next(read_samples(maps_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
